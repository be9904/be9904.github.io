---
layout: post
title: 6. Optimizing Gradient Descent
date: Sat Oct  4 18:06:47 KST 2025
description: Overview of gradient descent optimization techniques covered in the course Introduction to Deep Neural Networks (SWE3052)
img: Deep-Learning-neural-network.png
fig-caption: # Add figcaption (optional)
tags: [Artificial Intelligence, Deep Neural Networks, Gradient Descent, Optimization]
visibility: public
bmac: hide
---
# Contents
1. [Stochastic Gradient Descent](<#stochastic-gradient-descent>)
2. [Momentum](#momentum)
3. [Adaptive Learning Rates](<#adaptive-learning-rates>)
4. [References](#references)

# Stochastic Gradient Descent
Stochastic gradient descent is a method that accelerates training. For simple models that have few layers and few neurons per cell, vanilla gradient descent(aka batch gradient descent) will run without causing a lot of issues. The problem arises when the scale of the model gets large and complicated. Let's assume we apply vanilla gradient descent to a neural network that has a layer with 10,000 features. Weight update would look something like this.

\\[
    w_1 = w_1 - \eta\frac{\partial}{\partial w_1}\sum_{i=1}^n(t_i-y_i)^2
    = w_1 - \eta(\frac{\partial E_1}{\partial w_1} + \frac{\partial E_2}{\partial w_1} 
    + ... + \frac{\partial E_n}{\partial w_1})
\\]
\\[
    w_2 = w_2 - \eta\frac{\partial}{\partial w_2}\sum_{i=1}^n(t_i-y_i)^2
    = w_2 - \eta(\frac{\partial E_1}{\partial w_2} + \frac{\partial E_2}{\partial w_2} 
    + ... + \frac{\partial E_n}{\partial w_2})
\\]
\\[...\\]
\\[
    w_{10000} = w_{10000} - \eta\frac{\partial}{\partial w_{10000}}\sum_{i=1}^n(t_i-y_i)^2
    = w_{10000} - \eta(\frac{\partial E_1}{\partial w_{10000}} + \frac{\partial E_2}{\partial w_{10000}} 
    + ... + \frac{\partial E_n}{\partial w_{10000}})
\\]
<p align="center"><b><i>Vanilla Gradient Descent</i></b></p>

This is the computation for 1 sample. But if we have 10,000 samples, this would mean we would need 100 million calculations just for one epoch and will take too much time to train.

Stochastic gradient descent addresses this by randomly sampling 1 gradient. Mini batch gradient descent uses a small batch of gradients(usually \\(2^n\\) samples) in each epoch. This statistic approach allows us to estimate the gradient with less computation.

*There is some confusion in terminology, but generally, batch gradient descent refers to the standard method that computes gradients over the entire dataset, while stochastic and mini-batch gradient descent refer to methods that compute gradients using a batch size of 1 and n (16, 32, 64, â€¦), respectively.*

Since we know that
\\[
    \frac{\partial E}{\partial w_1} = \frac{1}{n}\sum_{i=1}^{n}\frac{\partial E_i}{\partial w_1}
\\]
we can assume
\\[
    \frac{\partial E}{\partial w_1}\approx\frac{\partial E_1}{\partial w_1}\approx\frac{\partial E_2}{\partial w_1}\approx...\approx\frac{\partial E_n}{\partial w_1}
\\]
and use this value to substitute the entire sum of gradients.

The pseudo code for stochastic gradient descent is as follows.
```python
Repeat
    for n=1 to N(Entire training dataset)
        w = w - lr * dE_n / dw
    end
```
This is usually faster than vanilla gradient descent and can also be used for online learning. However, since it is just 1 sample of the entire set, fluctuations can be present and affect accuracy negatively. Using a small learning rate can be a way to achieve similar performance.

Mini batch gradient descent uses batches of size n (typically numbers that are \\(2^n\\)). For 1 epoch, gradients are calculated for each batch.
```python
Repeat
    for b=1 to B(All batches)
        g_sum = 0 # gradient sum
        for n=1 to N(Entire training dataset)
            g_sum += dE_n/dw
        end
        w = w - lr * g_sum
    end
```
One notable part of the process is that different gradient values are used for each batch. For example, if we set the batch size as 2, the first batch uses \\(\frac{\partial E_1}{\partial w_m} + \frac{\partial E_2}{\partial w_m}\\), the next batch uses \\(\frac{\partial E_3}{\partial w_m} + \frac{\partial E_4}{\partial w_m}\\), and so on. This is because using the same value for every batch will lead to biased results, consequently degrading accuracy.

Both stochastic gradient descent and mini batch gradient descent provide a good estimation of the real gradient and allow high throughput, allowing a large number of GPU cores to be used at once, which enables fast convergence. However, when datasets have high variance, the results may be noisy and inaccurate. Mini batch gradient descent (n samples) usually yields better results and is more stable than stochastic gradient descent (1 sample), but it still differs from batch gradient descent (all samples) in terms of gradient values.

# Momentum

Naive gradient descent methods can be very useful, but has the disadvantage of not being able to train when the gradient is zero or close to zero. If a point is near a local minimum, small gradients are not a problem. However, the issue arises when the gradient is close to zero at a point far from any minimum. A common scenario is when a randomly initialized starting point produces very small gradients, which can stall gradient-based optimization.

This is where momentum can be applied. Momentum is a technique that improves optimization quality. It can be thought of as a ball that rolls down the loss function. A simple gradient descent method will stop as soon as it reaches a local minimum, but with momentum applied, the point will keep moving even after reaching a local minimum and find a better minimum value.
<p align="center">
  <img src="/assets/img/2025-10-19-gdo/momentum.gif" width="80%" height="80%">
 </p>
<p align="center"><b><i>SGD and SGD with Momentum</i></b></p>
Momentum is defined by adding a vector in the direction of past weight updates, combining the previous update with the current gradient.
\\[
    m^t = \gamma m^{t-1} - \eta g^t
\\]
\\[   
    w^t = w^{t-1} + m^t
\\]
- \\(g^t\\): gradient at iteration t
- \\(\gamma\\): momentum rate
- \\(\eta\\): learning rate

Momentum can be written as follows.
\\[
    m^t = -\eta(g^t +\gamma g^{t-1} + \gamma^2g^{t-2} + ... + \gamma^{t-1}g^1)    
\\]
This is equivalent to saying momentum is the **exponential average** of past gradients. 
With momentum, the learning process becomes more stable and consistent.

One advantage of momentum is that it helps **escape local minima**(although not guaranteed). If we assume a scenario where \\(g^t\\) is a value close to 0,
\\[
    m^t = -\eta(g^t + \gamma g^{t-1} + \gamma^2g^{t-2} + ... + \gamma^{t-1}g^1)
\\]
\\[
    = -\eta(\gamma g^{t-1} + \gamma^2g^{t-2} + ... + \gamma^{t-1}g^1)
\\]
We can see that even though \\(g^t\\) is 0, the past values have been accumulated and is not 0 meaning it will still update and converge to some local minimum.

Another advantage is that it allows **faster convergence**. As mentioned above, gradients are accumulated exponentially, which results in:
1. Momentum being amplified in areas where gradients directions are consistent, e.g. shallow directions where gradient values are close to 0.
2. Reduced oscillation in steep directions because gradients with alternating signs cancel each other out.
3. A larger learning rate along consistent directions due to exponential accumulation of past gradients.

<p align="center">
  <img src="/assets/img/2025-10-19-gdo/momentum2.gif" width="80%" height="80%">
 </p>
<p align="center"><b><i>SGD and SGD with Momentum</i></b></p>

# Adaptive Learning Rates

# References
[Optimization with Momentum](https://gbhat.com/machine_learning/optimize_with_momentum.html)

[Gradient Descent Nesterov Momentum](https://gbhat.com/machine_learning/gradient_descent_nesterov.html)