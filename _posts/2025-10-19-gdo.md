---
layout: post
title: 6. Gradient Descent Optimizer
date: Sat Oct  4 18:06:47 KST 2025
description: Overview of gradient descent optimization techniques covered in the course Introduction to Deep Neural Networks (SWE3052)
img: Deep-Learning-neural-network.png
fig-caption: # Add figcaption (optional)
tags: [Artificial Intelligence, Deep Neural Networks, Gradient Descent, Optimization]
visibility: public
bmac: hide
---
# Contents
1. [Stochastic Gradient Descent](<#stochastic-gradient-descent>)
2. [Momentum](#momentum)
3. [Adaptive Learning Rates](<#adaptive-learning-rates>)
4. [References](#references)

# Stochastic Gradient Descent
Stochastic gradient descent is a method that accelerates training. For simple models that have few layers and few neurons per cell, vanilla gradient descent(aka batch gradient descent) will run without causing a lot of issues. The problem arises when the scale of the model gets large and complicated. Let's assume we apply vanilla gradient descent to a neural network that has a layer with 10,000 features. Weight update would look something like this.

\\[
    w_1 = w_1 - \eta\frac{\partial}{\partial w_1}\sum_{i=1}^n(t_i-y_i)^2
    = w_1 - \eta(\frac{\partial E_1}{\partial w_1} + \frac{\partial E_2}{\partial w_1} 
    + ... + \frac{\partial E_n}{\partial w_1})
\\]
\\[
    w_2 = w_2 - \eta\frac{\partial}{\partial w_2}\sum_{i=1}^n(t_i-y_i)^2
    = w_2 - \eta(\frac{\partial E_1}{\partial w_2} + \frac{\partial E_2}{\partial w_2} 
    + ... + \frac{\partial E_n}{\partial w_2})
\\]
\\[...\\]
\\[
    w_{10000} = w_{10000} - \eta\frac{\partial}{\partial w_{10000}}\sum_{i=1}^n(t_i-y_i)^2
    = w_{10000} - \eta(\frac{\partial E_1}{\partial w_{10000}} + \frac{\partial E_2}{\partial w_{10000}} 
    + ... + \frac{\partial E_n}{\partial w_{10000}})
\\]
<p align="center"><b><i>Vanilla Gradient Descent</i></b></p>

This is the computation for 1 sample. But if we have 10,000 samples, this would mean we would need 100 million calculations just for one epoch and will take too much time to train.

Stochastic gradient descent addresses this by randomly sampling 1 gradient. Mini batch gradient descent uses a small batch of gradients(usually \\(2^n\\) samples) in each epoch. This statistic approach allows us to estimate the gradient with less computation.

*There is some confusion in terminology, but generally, batch gradient descent refers to the standard method that computes gradients over the entire dataset, while stochastic and mini-batch gradient descent refer to methods that compute gradients using a batch size of 1 and n (16, 32, 64, â€¦), respectively.*

Since we know that
\\[
    \frac{\partial E}{\partial w_1} = \frac{1}{n}\sum_{i=1}^{n}\frac{\partial E_i}{\partial w_1}
\\]
we can assume
\\[
    \frac{\partial E}{\partial w_1}\approx\frac{\partial E_1}{\partial w_1}\approx\frac{\partial E_2}{\partial w_1}\approx...\approx\frac{\partial E_n}{\partial w_1}
\\]
and use this value to substitute the entire sum of gradients.

The pseudo code for stochastic gradient descent is as follows.
```python
Repeat
    for n=1 to N(Entire training dataset)
        w = w - lr * dE_n / dw
    end
```
This is usually faster than vanilla gradient descent and can also be used for online learning. However, since it is just 1 sample of the entire set, fluctuations can be present and affect accuracy negatively. Using a small learning rate can be a way to achieve similar performance.

Mini batch gradient descent uses batches of size n (typically numbers that are \\(2^n\\)). For 1 epoch, gradients are calculated for each batch.
```python
Repeat
    for b=1 to B(All batches)
        g_sum = 0 # gradient sum
        for n=1 to N(Entire training dataset)
            g_sum += dE_n/dw
        end
        w = w - lr * g_sum
    end
```
One notable part of the process is that different gradient values are used for each batch. For example, if we set the batch size as 2, the first batch uses $\frac{\partial E_1}{\partial w_m} + \frac{\partial E_2}{\partial w_m}$, the next batch uses $\frac{\partial E_3}{\partial w_m} + \frac{\partial E_4}{\partial w_m}$, and so on. This is because using the same value for every batch will lead to biased results, consequently degrading accuracy.

Mini batch gradient descent(n samples) usually yields better results compared to stochastic gradient descent(1 sample), but it still differs from batch gradient descent(all samples) in terms of gradient values.

# Momentum

# Adaptive Learning Rates

# References
[Optimization with Momentum](https://gbhat.com/machine_learning/optimize_with_momentum.html)