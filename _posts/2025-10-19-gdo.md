---
layout: post
title: 6. Gradient Descent Optimizer
date: Sat Oct  4 18:06:47 KST 2025
description: Overview of gradient descent optimization techniques covered in the course Introduction to Deep Neural Networks (SWE3052)
img: Deep-Learning-neural-network.png
fig-caption: # Add figcaption (optional)
tags: [Artificial Intelligence, Deep Neural Networks, Gradient Descent, Optimization]
visibility: public
bmac: hide
---
# Contents
1. [Stochastic Gradient Descent](<#stochastic-gradient-descent>)
2. [Momentum](#momentum)
3. [Adaptive Learning Rates](<#adaptive-learning-rates>)
4. [References](#references)

# Stochastic Gradient Descent
Stochastic gradient descent is a method that accelerates training. For simple models that have few layers and few neurons per cell, vanilla gradient descent(aka batch gradient descent) will run without causing a lot of issues. The problem arises when the scale of the model gets large and complicated. Let's assume we apply vanilla gradient descent to a neural network that has a layer with 10,000 features. Weight update would look something like this.

\\[
    w_1 = w_1 - \eta\frac{\partial}{\partial w_1}\sum_{i=1}^n(t_i-y_i)^2
    = w_1 - \eta(\frac{\partial E_1}{\partial w_1} + \frac{\partial E_2}{\partial w_1} 
    + ... + \frac{\partial E_n}{\partial w_1})
\\]
\\[
    w_2 = w_2 - \eta\frac{\partial}{\partial w_2}\sum_{i=1}^n(t_i-y_i)^2
    = w_2 - \eta(\frac{\partial E_1}{\partial w_2} + \frac{\partial E_2}{\partial w_2} 
    + ... + \frac{\partial E_n}{\partial w_2})
\\]
\\[...\\]
\\[
    w_{10000} = w_{10000} - \eta\frac{\partial}{\partial w_{10000}}\sum_{i=1}^n(t_i-y_i)^2
    = w_{10000} - \eta(\frac{\partial E_1}{\partial w_{10000}} + \frac{\partial E_2}{\partial w_{10000}} 
    + ... + \frac{\partial E_n}{\partial w_{10000}})
\\]
<p align="center"><b><i>Vanilla Gradient Descent</i></b></p>

This is the computation for 1 sample. But if we have 10,000 samples, this would mean we would need 100 million calculations just for one epoch and will take too much time to train.

Stochastic gradient descent addresses this by randomly sampling 1 gradient. Mini batch gradient descent uses a small batch of gradients(usually $2^n$ samples) in each epoch. This statistic approach allows us to estimate the gradient with less computation.

*There is some confusion in terminology, but generally, batch gradient descent refers to the standard method that computes gradients over the entire dataset, while stochastic and mini-batch gradient descent refer to methods that compute gradients using only a small subset of examples. Some textbooks define stochastic gradient descent specifically as the case where the batch size is 1.*

Since we know that
\\[
    \frac{\partial E}{\partial w_1} = \frac{1}{n}\sum_{i=1}^{n}\frac{\partial E_i}{\partial w_1}
\\]
we can assume
\\[
    \frac{\partial E}{\partial w_1}\approx\frac{\partial E_1}{\partial w_1}\approx\frac{\partial E_2}{\partial w_1}\approx...\approx\frac{\partial E_n}{\partial w_1}
\\]



# Momentum

# Adaptive Learning Rates

# References
[Optimization with Momentum](https://gbhat.com/machine_learning/optimize_with_momentum.html)